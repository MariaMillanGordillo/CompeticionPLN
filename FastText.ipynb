{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "osVFlKkRorlT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos de entrenamiento y prueba\n",
        "train_path = \"FinancES_train_kaggle.csv\"  # Ruta del archivo de entrenamiento\n",
        "test_path = \"FinancES_test_kaggle.csv\"    # Ruta del archivo de prueba\n",
        "\n",
        "# Leer los archivos CSV\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# Mostrar información de los datos cargados\n",
        "print(\"Train Dataset:\")\n",
        "print(train_df.info())\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nTest Dataset:\")\n",
        "print(test_df.info())\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR2AZoXMo04Y",
        "outputId": "06359ff0-cf18-4fe8-958d-d89265af1018"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6359 entries, 0 to 6358\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      6359 non-null   int64 \n",
            " 1   text    6359 non-null   object\n",
            " 2   label   6359 non-null   int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 149.2+ KB\n",
            "None\n",
            "   id                                               text  label\n",
            "0   0  Renfe afronta mañana un nuevo día de paros par...      2\n",
            "1   1       Presupuesto populista con cimientos frágiles      2\n",
            "2   2  Biden no cree que la OPEP+ vaya a ayudar con l...      2\n",
            "3   3  La deuda de las familias cae en 25.000 millone...      0\n",
            "4   4  Bestinver: no hay \"momento más inoportuno\" par...      2\n",
            "\n",
            "Test Dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1621 entries, 0 to 1620\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      1621 non-null   int64 \n",
            " 1   text    1621 non-null   object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 25.5+ KB\n",
            "None\n",
            "   id                                               text\n",
            "0   0  Las empresas chinas piden menos burocracia par...\n",
            "1   1  Enrique Escalante (Havas): “Todos tenemos que ...\n",
            "2   2  Banca March confía su gestión empresarial a la...\n",
            "3   3  Garamendi, sobre la ruptura de la fusión: \"Hay...\n",
            "4   4  El Gobierno vasco da por hecho la continuidad ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalización: convertir a minúsculas\n",
        "train_df[\"text\"] = train_df[\"text\"].str.lower()\n",
        "test_df[\"text\"] = test_df[\"text\"].str.lower()"
      ],
      "metadata": {
        "id": "kqg5XcHhpRhb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Contar palabras más frecuentes en el dataset\n",
        "all_words = \" \".join(train_df[\"text\"]).split()\n",
        "word_freq = Counter(all_words)\n",
        "\n",
        "# Eliminar palabras que aparecen en más del 80% de los documentos\n",
        "frequent_words = {word for word, freq in word_freq.items() if freq > 0.8 * len(train_df)}\n",
        "\n",
        "# Inicializar el Stemmer para español\n",
        "stemmer = SnowballStemmer(\"spanish\")\n",
        "\n",
        "# Lista de stopwords personalizada\n",
        "nltk.download('stopwords')\n",
        "custom_stopwords = set(stopwords.words('spanish'))\n",
        "\n",
        "# Agregar palabras irrelevantes para titulares financieros\n",
        "custom_stopwords.update({\"día\", \"años\", \"mes\", \"nuevo\", \"euros\"})\n",
        "\n",
        "# Añadir palabras frecuentes a la lista de stopwords\n",
        "custom_stopwords.update(frequent_words)\n",
        "\n",
        "# Función de preprocesamiento con Stemming y stopwords mejoradas\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Tokenización\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Eliminar signos de puntuación y caracteres especiales, manteniendo números\n",
        "    tokens = [re.sub(r\"(?<!\\d)[^\\w\\s](?!\\d)\", \"\", token) for token in tokens]\n",
        "\n",
        "    # Eliminar stopwords personalizadas\n",
        "    tokens = [word for word in tokens if word not in custom_stopwords]\n",
        "\n",
        "    # Aplicar Stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Aplicar preprocesamiento\n",
        "train_df[\"text\"] = train_df[\"text\"].apply(preprocess_text)\n",
        "test_df[\"text\"] = test_df[\"text\"].apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLeSdgdOuUZ0",
        "outputId": "b11ddf46-cf3e-4e7f-9659-dba5cc2207ba"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTexto preprocesado:\")\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J0QmNUYwkW9",
        "outputId": "3494b545-4c4b-43b5-8ace-2c322ba0bdce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texto preprocesado:\n",
            "   id                                               text  label\n",
            "0   0               renf afront mañan par parcial maquin      2\n",
            "1   1                    presupuest popul cimient fragil      2\n",
            "2   2                bid cre opep vay ayud preci petrole      2\n",
            "3   3  deud famili cae 25.000 millon 2015 marc nivel ...      0\n",
            "4   4                 bestinv   moment inoportun  brexit      2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText\n"
      ],
      "metadata": {
        "id": "mAttgCwIwBx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DCauRVOwLuT",
        "outputId": "d5a6eae8-d0b7-4357-e636-496b0dc83116"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313470 sha256=454140a7f9063565d828bd56792ad911247a472979203108fcf70ba3ea7db925\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapear sentimiento a formato de FastText\n",
        "train_df['label'] = train_df['label'].astype(str).map({'0': \"__label__0\",\n",
        "                                                       '1': \"__label__1\",\n",
        "                                                       '2': \"__label__2\"})\n",
        "\n",
        "print(train_df.head())\n",
        "# Guardar en formato FastText\n",
        "train_frac = 0.8\n",
        "train_data = train_df.sample(frac=train_frac, random_state=42)\n",
        "test_data = train_df.drop(train_data.index)\n",
        "\n",
        "train_data[['label','text']].to_csv(\"train.txt\", index=False, sep='\\t', header=False, quoting=3, escapechar='\\\\')\n",
        "test_data[['label','text']].to_csv(\"test.txt\", index=False, sep='\\t', header=False, quoting=3, escapechar='\\\\')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNyWNA-UwWmA",
        "outputId": "78a51062-f8c5-4029-f19c-4c4dbd1a6798"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                               text       label\n",
            "0   0               renf afront mañan par parcial maquin  __label__2\n",
            "1   1                    presupuest popul cimient fragil  __label__2\n",
            "2   2                bid cre opep vay ayud preci petrole  __label__2\n",
            "3   3  deud famili cae 25.000 millon 2015 marc nivel ...  __label__0\n",
            "4   4                 bestinv   moment inoportun  brexit  __label__2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Entrenar el modelo\n",
        "model = fasttext.train_supervised(input=\"train.txt\", epoch=100, lr=0.5, wordNgrams=2, verbose=2, minCount=1)\n",
        "\n",
        "# Guardar el modelo para uso posterior\n",
        "model.save_model(\"fasttext_sentiment.bin\")\n"
      ],
      "metadata": {
        "id": "umXXaI07y9JI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo con datos de prueba\n",
        "result = model.test(\"test.txt\")\n",
        "print(f\"Precisión: {result[1] * 100:.2f}%\")  # result[1] es la precisión del modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-Gsa9qQzCT2",
        "outputId": "b974712f-0b32-4738-c0de-be1fe5b03aab"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión: 67.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = fasttext.load_model(\"fasttext_sentiment.bin\")\n",
        "\n",
        "# Cargar los datos de prueba\n",
        "test_data = pd.read_csv(\"test.txt\", sep='\\t', header=None, names=[\"label\", \"text\"], quoting=3)\n",
        "\n",
        "# Remover el prefijo \"__label__\" de las etiquetas reales\n",
        "test_data['label'] = test_data['label'].str.replace(\"__label__\", \"\")\n",
        "\n",
        "# Convertir etiquetas a formato numérico\n",
        "test_data['label'] = test_data['label'].astype(int)\n",
        "\n",
        "# Generar predicciones para cada texto\n",
        "predictions = [model.predict(text)[0][0].replace(\"__label__\", \"\") for text in test_data[\"text\"]]\n",
        "\n",
        "# Convertir predicciones a formato numérico\n",
        "predictions = list(map(int, predictions))\n",
        "\n",
        "# Calcular F1-score\n",
        "f1 = f1_score(test_data['label'], predictions, average='macro')\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOXQqmnczr5b",
        "outputId": "1c67582a-5e47-44c7-c5cd-b171059496c3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score: 0.6075\n"
          ]
        }
      ]
    }
  ]
}